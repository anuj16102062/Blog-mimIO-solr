{
  "blogs": [
    {
      "id": 150,
      "title": "Algorithms for Core Data Analytics and Machine Learning 150",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-09-12T00:00:00.000Z"
    },
    {
      "id": 151,
      "title": "Algorithms for Core Data Analytics and Machine Learning 151",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-12-29T00:00:00.000Z"
    },
    {
      "id": 152,
      "title": "Algorithms for Core Data Analytics and Machine Learning 152",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-07-02T00:00:00.000Z"
    },
    {
      "id": 153,
      "title": "Algorithms for Core Data Analytics and Machine Learning 153",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-05-17T00:00:00.000Z"
    },
    {
      "id": 154,
      "title": "Algorithms for Core Data Analytics and Machine Learning 154",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-03-02T00:00:00.000Z"
    },
    {
      "id": 155,
      "title": "Algorithms for Core Data Analytics and Machine Learning 155",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-07-14T00:00:00.000Z"
    },
    {
      "id": 156,
      "title": "Algorithms for Core Data Analytics and Machine Learning 156",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-09-26T00:00:00.000Z"
    },
    {
      "id": 157,
      "title": "Algorithms for Core Data Analytics and Machine Learning 157",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-09-28T00:00:00.000Z"
    },
    {
      "id": 158,
      "title": "Algorithms for Core Data Analytics and Machine Learning 158",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-10-23T00:00:00.000Z"
    },
    {
      "id": 159,
      "title": "Algorithms for Core Data Analytics and Machine Learning 159",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-12-21T00:00:00.000Z"
    },
    {
      "id": 160,
      "title": "Algorithms for Core Data Analytics and Machine Learning 160",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-07-29T00:00:00.000Z"
    },
    {
      "id": 161,
      "title": "Algorithms for Core Data Analytics and Machine Learning 161",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-01-19T00:00:00.000Z"
    },
    {
      "id": 162,
      "title": "Algorithms for Core Data Analytics and Machine Learning 162",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-10-16T00:00:00.000Z"
    },
    {
      "id": 163,
      "title": "Algorithms for Core Data Analytics and Machine Learning 163",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-12-06T00:00:00.000Z"
    },
    {
      "id": 164,
      "title": "Algorithms for Core Data Analytics and Machine Learning 164",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-08-20T00:00:00.000Z"
    },
    {
      "id": 165,
      "title": "Algorithms for Core Data Analytics and Machine Learning 165",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-06-28T00:00:00.000Z"
    },
    {
      "id": 166,
      "title": "Algorithms for Core Data Analytics and Machine Learning 166",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-09-19T00:00:00.000Z"
    },
    {
      "id": 167,
      "title": "Algorithms for Core Data Analytics and Machine Learning 167",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-12-20T00:00:00.000Z"
    },
    {
      "id": 168,
      "title": "Algorithms for Core Data Analytics and Machine Learning 168",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-02-12T00:00:00.000Z"
    },
    {
      "id": 169,
      "title": "Algorithms for Core Data Analytics and Machine Learning 169",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-11-22T00:00:00.000Z"
    },
    {
      "id": 170,
      "title": "Algorithms for Core Data Analytics and Machine Learning 170",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-01-22T00:00:00.000Z"
    },
    {
      "id": 171,
      "title": "Algorithms for Core Data Analytics and Machine Learning 171",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-07-11T00:00:00.000Z"
    },
    {
      "id": 172,
      "title": "Algorithms for Core Data Analytics and Machine Learning 172",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-08-04T00:00:00.000Z"
    },
    {
      "id": 173,
      "title": "Algorithms for Core Data Analytics and Machine Learning 173",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-05-08T00:00:00.000Z"
    },
    {
      "id": 174,
      "title": "Algorithms for Core Data Analytics and Machine Learning 174",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-02-25T00:00:00.000Z"
    },
    {
      "id": 175,
      "title": "Algorithms for Core Data Analytics and Machine Learning 175",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-12-01T00:00:00.000Z"
    },
    {
      "id": 176,
      "title": "Algorithms for Core Data Analytics and Machine Learning 176",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-11-01T00:00:00.000Z"
    },
    {
      "id": 177,
      "title": "Algorithms for Core Data Analytics and Machine Learning 177",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-10-25T00:00:00.000Z"
    },
    {
      "id": 178,
      "title": "Algorithms for Core Data Analytics and Machine Learning 178",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-07-19T00:00:00.000Z"
    },
    {
      "id": 179,
      "title": "Algorithms for Core Data Analytics and Machine Learning 179",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-01-12T00:00:00.000Z"
    },
    {
      "id": 180,
      "title": "Algorithms for Core Data Analytics and Machine Learning 180",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-04-08T00:00:00.000Z"
    },
    {
      "id": 181,
      "title": "Algorithms for Core Data Analytics and Machine Learning 181",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-04-30T00:00:00.000Z"
    },
    {
      "id": 182,
      "title": "Algorithms for Core Data Analytics and Machine Learning 182",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-05-01T00:00:00.000Z"
    },
    {
      "id": 183,
      "title": "Algorithms for Core Data Analytics and Machine Learning 183",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-04-25T00:00:00.000Z"
    },
    {
      "id": 184,
      "title": "Algorithms for Core Data Analytics and Machine Learning 184",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-12-05T00:00:00.000Z"
    },
    {
      "id": 185,
      "title": "Algorithms for Core Data Analytics and Machine Learning 185",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-09-05T00:00:00.000Z"
    },
    {
      "id": 186,
      "title": "Algorithms for Core Data Analytics and Machine Learning 186",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-05-23T00:00:00.000Z"
    },
    {
      "id": 187,
      "title": "Algorithms for Core Data Analytics and Machine Learning 187",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-01-20T00:00:00.000Z"
    },
    {
      "id": 188,
      "title": "Algorithms for Core Data Analytics and Machine Learning 188",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-06-23T00:00:00.000Z"
    },
    {
      "id": 189,
      "title": "Algorithms for Core Data Analytics and Machine Learning 189",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-08-23T00:00:00.000Z"
    },
    {
      "id": 190,
      "title": "Algorithms for Core Data Analytics and Machine Learning 190",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-11-11T00:00:00.000Z"
    },
    {
      "id": 191,
      "title": "Algorithms for Core Data Analytics and Machine Learning 191",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-11-06T00:00:00.000Z"
    },
    {
      "id": 192,
      "title": "Algorithms for Core Data Analytics and Machine Learning 192",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-10-03T00:00:00.000Z"
    },
    {
      "id": 193,
      "title": "Algorithms for Core Data Analytics and Machine Learning 193",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-07-24T00:00:00.000Z"
    },
    {
      "id": 194,
      "title": "Algorithms for Core Data Analytics and Machine Learning 194",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-09-23T00:00:00.000Z"
    },
    {
      "id": 195,
      "title": "Algorithms for Core Data Analytics and Machine Learning 195",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-04-30T00:00:00.000Z"
    },
    {
      "id": 196,
      "title": "Algorithms for Core Data Analytics and Machine Learning 196",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-12-21T00:00:00.000Z"
    },
    {
      "id": 197,
      "title": "Algorithms for Core Data Analytics and Machine Learning 197",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-08-14T00:00:00.000Z"
    },
    {
      "id": 198,
      "title": "Algorithms for Core Data Analytics and Machine Learning 198",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-09-27T00:00:00.000Z"
    },
    {
      "id": 199,
      "title": "Algorithms for Core Data Analytics and Machine Learning 199",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-07-02T00:00:00.000Z"
    },
    {
      "id": 200,
      "title": "Algorithms for Core Data Analytics and Machine Learning 200",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-02-22T00:00:00.000Z"
    },
    {
      "id": 201,
      "title": "Algorithms for Core Data Analytics and Machine Learning 201",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-06-17T00:00:00.000Z"
    },
    {
      "id": 202,
      "title": "Algorithms for Core Data Analytics and Machine Learning 202",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-08-22T00:00:00.000Z"
    },
    {
      "id": 203,
      "title": "Algorithms for Core Data Analytics and Machine Learning 203",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-05-26T00:00:00.000Z"
    },
    {
      "id": 204,
      "title": "Algorithms for Core Data Analytics and Machine Learning 204",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-02-13T00:00:00.000Z"
    },
    {
      "id": 205,
      "title": "Algorithms for Core Data Analytics and Machine Learning 205",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-12-13T00:00:00.000Z"
    },
    {
      "id": 206,
      "title": "Algorithms for Core Data Analytics and Machine Learning 206",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-09-27T00:00:00.000Z"
    },
    {
      "id": 207,
      "title": "Algorithms for Core Data Analytics and Machine Learning 207",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-11-10T00:00:00.000Z"
    },
    {
      "id": 208,
      "title": "Algorithms for Core Data Analytics and Machine Learning 208",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-05-24T00:00:00.000Z"
    },
    {
      "id": 209,
      "title": "Algorithms for Core Data Analytics and Machine Learning 209",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-02-09T00:00:00.000Z"
    },
    {
      "id": 210,
      "title": "Algorithms for Core Data Analytics and Machine Learning 210",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-06-12T00:00:00.000Z"
    },
    {
      "id": 211,
      "title": "Algorithms for Core Data Analytics and Machine Learning 211",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-01-01T00:00:00.000Z"
    },
    {
      "id": 212,
      "title": "Algorithms for Core Data Analytics and Machine Learning 212",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-01-09T00:00:00.000Z"
    },
    {
      "id": 213,
      "title": "Algorithms for Core Data Analytics and Machine Learning 213",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-06-27T00:00:00.000Z"
    },
    {
      "id": 214,
      "title": "Algorithms for Core Data Analytics and Machine Learning 214",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-08-01T00:00:00.000Z"
    },
    {
      "id": 215,
      "title": "Algorithms for Core Data Analytics and Machine Learning 215",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-06-29T00:00:00.000Z"
    },
    {
      "id": 216,
      "title": "Algorithms for Core Data Analytics and Machine Learning 216",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-03-05T00:00:00.000Z"
    },
    {
      "id": 217,
      "title": "Algorithms for Core Data Analytics and Machine Learning 217",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-06-24T00:00:00.000Z"
    },
    {
      "id": 218,
      "title": "Algorithms for Core Data Analytics and Machine Learning 218",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-03-17T00:00:00.000Z"
    },
    {
      "id": 219,
      "title": "Algorithms for Core Data Analytics and Machine Learning 219",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-06-04T00:00:00.000Z"
    },
    {
      "id": 220,
      "title": "Algorithms for Core Data Analytics and Machine Learning 220",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-08-02T00:00:00.000Z"
    },
    {
      "id": 221,
      "title": "Algorithms for Core Data Analytics and Machine Learning 221",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-07-21T00:00:00.000Z"
    },
    {
      "id": 222,
      "title": "Algorithms for Core Data Analytics and Machine Learning 222",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-03-18T00:00:00.000Z"
    },
    {
      "id": 223,
      "title": "Algorithms for Core Data Analytics and Machine Learning 223",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-01-27T00:00:00.000Z"
    },
    {
      "id": 224,
      "title": "Algorithms for Core Data Analytics and Machine Learning 224",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-05-25T00:00:00.000Z"
    },
    {
      "id": 225,
      "title": "Algorithms for Core Data Analytics and Machine Learning 225",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-11-06T00:00:00.000Z"
    },
    {
      "id": 226,
      "title": "Algorithms for Core Data Analytics and Machine Learning 226",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-04-16T00:00:00.000Z"
    },
    {
      "id": 227,
      "title": "Algorithms for Core Data Analytics and Machine Learning 227",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-07-30T00:00:00.000Z"
    },
    {
      "id": 228,
      "title": "Algorithms for Core Data Analytics and Machine Learning 228",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-02-12T00:00:00.000Z"
    },
    {
      "id": 229,
      "title": "Algorithms for Core Data Analytics and Machine Learning 229",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-04-04T00:00:00.000Z"
    },
    {
      "id": 230,
      "title": "Algorithms for Core Data Analytics and Machine Learning 230",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-03-06T00:00:00.000Z"
    },
    {
      "id": 231,
      "title": "Algorithms for Core Data Analytics and Machine Learning 231",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-03-21T00:00:00.000Z"
    },
    {
      "id": 232,
      "title": "Algorithms for Core Data Analytics and Machine Learning 232",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-05-30T00:00:00.000Z"
    },
    {
      "id": 233,
      "title": "Algorithms for Core Data Analytics and Machine Learning 233",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-11-08T00:00:00.000Z"
    },
    {
      "id": 234,
      "title": "Algorithms for Core Data Analytics and Machine Learning 234",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-09-12T00:00:00.000Z"
    },
    {
      "id": 235,
      "title": "Algorithms for Core Data Analytics and Machine Learning 235",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-03-19T00:00:00.000Z"
    },
    {
      "id": 236,
      "title": "Algorithms for Core Data Analytics and Machine Learning 236",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-08-21T00:00:00.000Z"
    },
    {
      "id": 237,
      "title": "Algorithms for Core Data Analytics and Machine Learning 237",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-10-04T00:00:00.000Z"
    },
    {
      "id": 238,
      "title": "Algorithms for Core Data Analytics and Machine Learning 238",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-03-17T00:00:00.000Z"
    },
    {
      "id": 239,
      "title": "Algorithms for Core Data Analytics and Machine Learning 239",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-05-02T00:00:00.000Z"
    },
    {
      "id": 240,
      "title": "Algorithms for Core Data Analytics and Machine Learning 240",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-02-21T00:00:00.000Z"
    },
    {
      "id": 241,
      "title": "Algorithms for Core Data Analytics and Machine Learning 241",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-11-30T00:00:00.000Z"
    },
    {
      "id": 242,
      "title": "Algorithms for Core Data Analytics and Machine Learning 242",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-12-16T00:00:00.000Z"
    },
    {
      "id": 243,
      "title": "Algorithms for Core Data Analytics and Machine Learning 243",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-01-11T00:00:00.000Z"
    },
    {
      "id": 244,
      "title": "Algorithms for Core Data Analytics and Machine Learning 244",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-02-13T00:00:00.000Z"
    },
    {
      "id": 245,
      "title": "Algorithms for Core Data Analytics and Machine Learning 245",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-05-06T00:00:00.000Z"
    },
    {
      "id": 246,
      "title": "Algorithms for Core Data Analytics and Machine Learning 246",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-12-10T00:00:00.000Z"
    },
    {
      "id": 247,
      "title": "Algorithms for Core Data Analytics and Machine Learning 247",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-12-02T00:00:00.000Z"
    },
    {
      "id": 248,
      "title": "Algorithms for Core Data Analytics and Machine Learning 248",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-02-14T00:00:00.000Z"
    },
    {
      "id": 249,
      "title": "Algorithms for Core Data Analytics and Machine Learning 249",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-02-03T00:00:00.000Z"
    },
    {
      "id": 250,
      "title": "Algorithms for Core Data Analytics and Machine Learning 250",
      "content": "Part of the Data Foundation activity is to promote overall research to produce novel algorithms and improve existing algorithms for data analytics and machine learning. Of specific interest is the effective application of these algorithms to socially relevant applications. Another interesting research problem is to analyze data that is distributed or federated, and cannot be brought into a centralized place due to privacy/security considerations, or due to sheer volume.\n\n    While deep neural networks (DNNs) have achieved high performance on a variety of tasks, there is still scope for improvement, especially in applications where data is scarce, costly or restricted. Research of interest includes generating good quality synthetic datasets, data augmentation methods, attacks on DNNs and identifying deep fakes, making DNNs robust to attacks and identification by augmenting datasets, building interpretable methods, etc.",
      "publishedAt": "2023-09-11T00:00:00.000Z"
    },
    {
      "id": 500,
      "title": "Automation in Data Collection, Curation and Annotation 500",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-10-12T00:00:00.000Z"
    },
    {
      "id": 501,
      "title": "Automation in Data Collection, Curation and Annotation 501",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-07-20T00:00:00.000Z"
    },
    {
      "id": 502,
      "title": "Automation in Data Collection, Curation and Annotation 502",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-07-09T00:00:00.000Z"
    },
    {
      "id": 503,
      "title": "Automation in Data Collection, Curation and Annotation 503",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-01-13T00:00:00.000Z"
    },
    {
      "id": 504,
      "title": "Automation in Data Collection, Curation and Annotation 504",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-12-24T00:00:00.000Z"
    },
    {
      "id": 505,
      "title": "Automation in Data Collection, Curation and Annotation 505",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-10-22T00:00:00.000Z"
    },
    {
      "id": 506,
      "title": "Automation in Data Collection, Curation and Annotation 506",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-09-20T00:00:00.000Z"
    },
    {
      "id": 507,
      "title": "Automation in Data Collection, Curation and Annotation 507",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-08-18T00:00:00.000Z"
    },
    {
      "id": 508,
      "title": "Automation in Data Collection, Curation and Annotation 508",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-06-29T00:00:00.000Z"
    },
    {
      "id": 509,
      "title": "Automation in Data Collection, Curation and Annotation 509",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-09-29T00:00:00.000Z"
    },
    {
      "id": 510,
      "title": "Automation in Data Collection, Curation and Annotation 510",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-04-25T00:00:00.000Z"
    },
    {
      "id": 511,
      "title": "Automation in Data Collection, Curation and Annotation 511",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-08-16T00:00:00.000Z"
    },
    {
      "id": 512,
      "title": "Automation in Data Collection, Curation and Annotation 512",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-12-04T00:00:00.000Z"
    },
    {
      "id": 513,
      "title": "Automation in Data Collection, Curation and Annotation 513",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-02-19T00:00:00.000Z"
    },
    {
      "id": 514,
      "title": "Automation in Data Collection, Curation and Annotation 514",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-11-24T00:00:00.000Z"
    },
    {
      "id": 515,
      "title": "Automation in Data Collection, Curation and Annotation 515",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-09-01T00:00:00.000Z"
    },
    {
      "id": 516,
      "title": "Automation in Data Collection, Curation and Annotation 516",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-10-20T00:00:00.000Z"
    },
    {
      "id": 517,
      "title": "Automation in Data Collection, Curation and Annotation 517",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-09-07T00:00:00.000Z"
    },
    {
      "id": 518,
      "title": "Automation in Data Collection, Curation and Annotation 518",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-08-22T00:00:00.000Z"
    },
    {
      "id": 519,
      "title": "Automation in Data Collection, Curation and Annotation 519",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-01-09T00:00:00.000Z"
    },
    {
      "id": 520,
      "title": "Automation in Data Collection, Curation and Annotation 520",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-07-25T00:00:00.000Z"
    },
    {
      "id": 521,
      "title": "Automation in Data Collection, Curation and Annotation 521",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-02-26T00:00:00.000Z"
    },
    {
      "id": 522,
      "title": "Automation in Data Collection, Curation and Annotation 522",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-01-28T00:00:00.000Z"
    },
    {
      "id": 523,
      "title": "Automation in Data Collection, Curation and Annotation 523",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-09-08T00:00:00.000Z"
    },
    {
      "id": 524,
      "title": "Automation in Data Collection, Curation and Annotation 524",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-08-29T00:00:00.000Z"
    },
    {
      "id": 525,
      "title": "Automation in Data Collection, Curation and Annotation 525",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-05-13T00:00:00.000Z"
    },
    {
      "id": 526,
      "title": "Automation in Data Collection, Curation and Annotation 526",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-02-02T00:00:00.000Z"
    },
    {
      "id": 527,
      "title": "Automation in Data Collection, Curation and Annotation 527",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-12-27T00:00:00.000Z"
    },
    {
      "id": 528,
      "title": "Automation in Data Collection, Curation and Annotation 528",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-12-17T00:00:00.000Z"
    },
    {
      "id": 529,
      "title": "Automation in Data Collection, Curation and Annotation 529",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-08-08T00:00:00.000Z"
    },
    {
      "id": 530,
      "title": "Automation in Data Collection, Curation and Annotation 530",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-06-10T00:00:00.000Z"
    },
    {
      "id": 531,
      "title": "Automation in Data Collection, Curation and Annotation 531",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-08-25T00:00:00.000Z"
    },
    {
      "id": 532,
      "title": "Automation in Data Collection, Curation and Annotation 532",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-09-27T00:00:00.000Z"
    },
    {
      "id": 533,
      "title": "Automation in Data Collection, Curation and Annotation 533",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-07-29T00:00:00.000Z"
    },
    {
      "id": 534,
      "title": "Automation in Data Collection, Curation and Annotation 534",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-10-13T00:00:00.000Z"
    },
    {
      "id": 535,
      "title": "Automation in Data Collection, Curation and Annotation 535",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-08-19T00:00:00.000Z"
    },
    {
      "id": 536,
      "title": "Automation in Data Collection, Curation and Annotation 536",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-12-28T00:00:00.000Z"
    },
    {
      "id": 537,
      "title": "Automation in Data Collection, Curation and Annotation 537",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-10-27T00:00:00.000Z"
    },
    {
      "id": 538,
      "title": "Automation in Data Collection, Curation and Annotation 538",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-09-08T00:00:00.000Z"
    },
    {
      "id": 539,
      "title": "Automation in Data Collection, Curation and Annotation 539",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-05-19T00:00:00.000Z"
    },
    {
      "id": 540,
      "title": "Automation in Data Collection, Curation and Annotation 540",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-11-29T00:00:00.000Z"
    },
    {
      "id": 541,
      "title": "Automation in Data Collection, Curation and Annotation 541",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-11-28T00:00:00.000Z"
    },
    {
      "id": 542,
      "title": "Automation in Data Collection, Curation and Annotation 542",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-02-25T00:00:00.000Z"
    },
    {
      "id": 543,
      "title": "Automation in Data Collection, Curation and Annotation 543",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-11-02T00:00:00.000Z"
    },
    {
      "id": 544,
      "title": "Automation in Data Collection, Curation and Annotation 544",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-05-12T00:00:00.000Z"
    },
    {
      "id": 545,
      "title": "Automation in Data Collection, Curation and Annotation 545",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-04-22T00:00:00.000Z"
    },
    {
      "id": 546,
      "title": "Automation in Data Collection, Curation and Annotation 546",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-12-28T00:00:00.000Z"
    },
    {
      "id": 547,
      "title": "Automation in Data Collection, Curation and Annotation 547",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-06-27T00:00:00.000Z"
    },
    {
      "id": 548,
      "title": "Automation in Data Collection, Curation and Annotation 548",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-01-22T00:00:00.000Z"
    },
    {
      "id": 549,
      "title": "Automation in Data Collection, Curation and Annotation 549",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-03-18T00:00:00.000Z"
    },
    {
      "id": 550,
      "title": "Automation in Data Collection, Curation and Annotation 550",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-02-05T00:00:00.000Z"
    },
    {
      "id": 551,
      "title": "Automation in Data Collection, Curation and Annotation 551",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-06-03T00:00:00.000Z"
    },
    {
      "id": 552,
      "title": "Automation in Data Collection, Curation and Annotation 552",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-03-10T00:00:00.000Z"
    },
    {
      "id": 553,
      "title": "Automation in Data Collection, Curation and Annotation 553",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-05-21T00:00:00.000Z"
    },
    {
      "id": 554,
      "title": "Automation in Data Collection, Curation and Annotation 554",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-05-10T00:00:00.000Z"
    },
    {
      "id": 555,
      "title": "Automation in Data Collection, Curation and Annotation 555",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-03-13T00:00:00.000Z"
    },
    {
      "id": 556,
      "title": "Automation in Data Collection, Curation and Annotation 556",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-10-08T00:00:00.000Z"
    },
    {
      "id": 557,
      "title": "Automation in Data Collection, Curation and Annotation 557",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-08-15T00:00:00.000Z"
    },
    {
      "id": 558,
      "title": "Automation in Data Collection, Curation and Annotation 558",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-02-14T00:00:00.000Z"
    },
    {
      "id": 559,
      "title": "Automation in Data Collection, Curation and Annotation 559",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-12-15T00:00:00.000Z"
    },
    {
      "id": 560,
      "title": "Automation in Data Collection, Curation and Annotation 560",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-10-13T00:00:00.000Z"
    },
    {
      "id": 561,
      "title": "Automation in Data Collection, Curation and Annotation 561",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-11-16T00:00:00.000Z"
    },
    {
      "id": 562,
      "title": "Automation in Data Collection, Curation and Annotation 562",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-05-22T00:00:00.000Z"
    },
    {
      "id": 563,
      "title": "Automation in Data Collection, Curation and Annotation 563",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-04-26T00:00:00.000Z"
    },
    {
      "id": 564,
      "title": "Automation in Data Collection, Curation and Annotation 564",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-11-26T00:00:00.000Z"
    },
    {
      "id": 565,
      "title": "Automation in Data Collection, Curation and Annotation 565",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-03-22T00:00:00.000Z"
    },
    {
      "id": 566,
      "title": "Automation in Data Collection, Curation and Annotation 566",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-12-04T00:00:00.000Z"
    },
    {
      "id": 567,
      "title": "Automation in Data Collection, Curation and Annotation 567",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-04-03T00:00:00.000Z"
    },
    {
      "id": 568,
      "title": "Automation in Data Collection, Curation and Annotation 568",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-03-03T00:00:00.000Z"
    },
    {
      "id": 569,
      "title": "Automation in Data Collection, Curation and Annotation 569",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-04-23T00:00:00.000Z"
    },
    {
      "id": 570,
      "title": "Automation in Data Collection, Curation and Annotation 570",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-10-12T00:00:00.000Z"
    },
    {
      "id": 571,
      "title": "Automation in Data Collection, Curation and Annotation 571",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-05-27T00:00:00.000Z"
    },
    {
      "id": 572,
      "title": "Automation in Data Collection, Curation and Annotation 572",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-06-25T00:00:00.000Z"
    },
    {
      "id": 573,
      "title": "Automation in Data Collection, Curation and Annotation 573",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-08-23T00:00:00.000Z"
    },
    {
      "id": 574,
      "title": "Automation in Data Collection, Curation and Annotation 574",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-08-19T00:00:00.000Z"
    },
    {
      "id": 575,
      "title": "Automation in Data Collection, Curation and Annotation 575",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-05-23T00:00:00.000Z"
    },
    {
      "id": 576,
      "title": "Automation in Data Collection, Curation and Annotation 576",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-07-01T00:00:00.000Z"
    },
    {
      "id": 577,
      "title": "Automation in Data Collection, Curation and Annotation 577",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-06-24T00:00:00.000Z"
    },
    {
      "id": 578,
      "title": "Automation in Data Collection, Curation and Annotation 578",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-11-09T00:00:00.000Z"
    },
    {
      "id": 579,
      "title": "Automation in Data Collection, Curation and Annotation 579",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-08-07T00:00:00.000Z"
    },
    {
      "id": 580,
      "title": "Automation in Data Collection, Curation and Annotation 580",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-11-11T00:00:00.000Z"
    },
    {
      "id": 581,
      "title": "Automation in Data Collection, Curation and Annotation 581",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-06-13T00:00:00.000Z"
    },
    {
      "id": 582,
      "title": "Automation in Data Collection, Curation and Annotation 582",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-01-12T00:00:00.000Z"
    },
    {
      "id": 583,
      "title": "Automation in Data Collection, Curation and Annotation 583",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-02-10T00:00:00.000Z"
    },
    {
      "id": 584,
      "title": "Automation in Data Collection, Curation and Annotation 584",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-01-30T00:00:00.000Z"
    },
    {
      "id": 585,
      "title": "Automation in Data Collection, Curation and Annotation 585",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-10-31T00:00:00.000Z"
    },
    {
      "id": 586,
      "title": "Automation in Data Collection, Curation and Annotation 586",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-04-04T00:00:00.000Z"
    },
    {
      "id": 587,
      "title": "Automation in Data Collection, Curation and Annotation 587",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-06-27T00:00:00.000Z"
    },
    {
      "id": 588,
      "title": "Automation in Data Collection, Curation and Annotation 588",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-09-03T00:00:00.000Z"
    },
    {
      "id": 589,
      "title": "Automation in Data Collection, Curation and Annotation 589",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-07-24T00:00:00.000Z"
    },
    {
      "id": 590,
      "title": "Automation in Data Collection, Curation and Annotation 590",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-11-05T00:00:00.000Z"
    },
    {
      "id": 591,
      "title": "Automation in Data Collection, Curation and Annotation 591",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-08-18T00:00:00.000Z"
    },
    {
      "id": 592,
      "title": "Automation in Data Collection, Curation and Annotation 592",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-12-19T00:00:00.000Z"
    },
    {
      "id": 593,
      "title": "Automation in Data Collection, Curation and Annotation 593",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-09-04T00:00:00.000Z"
    },
    {
      "id": 594,
      "title": "Automation in Data Collection, Curation and Annotation 594",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-09-23T00:00:00.000Z"
    },
    {
      "id": 595,
      "title": "Automation in Data Collection, Curation and Annotation 595",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-07-21T00:00:00.000Z"
    },
    {
      "id": 596,
      "title": "Automation in Data Collection, Curation and Annotation 596",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-11-18T00:00:00.000Z"
    },
    {
      "id": 597,
      "title": "Automation in Data Collection, Curation and Annotation 597",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-02-28T00:00:00.000Z"
    },
    {
      "id": 598,
      "title": "Automation in Data Collection, Curation and Annotation 598",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-09-09T00:00:00.000Z"
    },
    {
      "id": 599,
      "title": "Automation in Data Collection, Curation and Annotation 599",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-04-21T00:00:00.000Z"
    },
    {
      "id": 600,
      "title": "Automation in Data Collection, Curation and Annotation 600",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-06-23T00:00:00.000Z"
    },
    {
      "id": 601,
      "title": "Automation in Data Collection, Curation and Annotation 601",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-11-18T00:00:00.000Z"
    },
    {
      "id": 602,
      "title": "Automation in Data Collection, Curation and Annotation 602",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-08-29T00:00:00.000Z"
    },
    {
      "id": 603,
      "title": "Automation in Data Collection, Curation and Annotation 603",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-11-18T00:00:00.000Z"
    },
    {
      "id": 604,
      "title": "Automation in Data Collection, Curation and Annotation 604",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-08-06T00:00:00.000Z"
    },
    {
      "id": 605,
      "title": "Automation in Data Collection, Curation and Annotation 605",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-01-24T00:00:00.000Z"
    },
    {
      "id": 606,
      "title": "Automation in Data Collection, Curation and Annotation 606",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-04-23T00:00:00.000Z"
    },
    {
      "id": 607,
      "title": "Automation in Data Collection, Curation and Annotation 607",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-10-12T00:00:00.000Z"
    },
    {
      "id": 608,
      "title": "Automation in Data Collection, Curation and Annotation 608",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-09-06T00:00:00.000Z"
    },
    {
      "id": 609,
      "title": "Automation in Data Collection, Curation and Annotation 609",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-08-29T00:00:00.000Z"
    },
    {
      "id": 610,
      "title": "Automation in Data Collection, Curation and Annotation 610",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-04-19T00:00:00.000Z"
    },
    {
      "id": 611,
      "title": "Automation in Data Collection, Curation and Annotation 611",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-01-28T00:00:00.000Z"
    },
    {
      "id": 612,
      "title": "Automation in Data Collection, Curation and Annotation 612",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-04-10T00:00:00.000Z"
    },
    {
      "id": 613,
      "title": "Automation in Data Collection, Curation and Annotation 613",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-09-09T00:00:00.000Z"
    },
    {
      "id": 614,
      "title": "Automation in Data Collection, Curation and Annotation 614",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-11-19T00:00:00.000Z"
    },
    {
      "id": 615,
      "title": "Automation in Data Collection, Curation and Annotation 615",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-08-06T00:00:00.000Z"
    },
    {
      "id": 616,
      "title": "Automation in Data Collection, Curation and Annotation 616",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-01-17T00:00:00.000Z"
    },
    {
      "id": 617,
      "title": "Automation in Data Collection, Curation and Annotation 617",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-05-17T00:00:00.000Z"
    },
    {
      "id": 618,
      "title": "Automation in Data Collection, Curation and Annotation 618",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-12-29T00:00:00.000Z"
    },
    {
      "id": 619,
      "title": "Automation in Data Collection, Curation and Annotation 619",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-07-27T00:00:00.000Z"
    },
    {
      "id": 620,
      "title": "Automation in Data Collection, Curation and Annotation 620",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-03-30T00:00:00.000Z"
    },
    {
      "id": 621,
      "title": "Automation in Data Collection, Curation and Annotation 621",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-03-12T00:00:00.000Z"
    },
    {
      "id": 622,
      "title": "Automation in Data Collection, Curation and Annotation 622",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-03-05T00:00:00.000Z"
    },
    {
      "id": 623,
      "title": "Automation in Data Collection, Curation and Annotation 623",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-10-30T00:00:00.000Z"
    },
    {
      "id": 624,
      "title": "Automation in Data Collection, Curation and Annotation 624",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-06-17T00:00:00.000Z"
    },
    {
      "id": 625,
      "title": "Automation in Data Collection, Curation and Annotation 625",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-10-15T00:00:00.000Z"
    },
    {
      "id": 626,
      "title": "Automation in Data Collection, Curation and Annotation 626",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-12-21T00:00:00.000Z"
    },
    {
      "id": 627,
      "title": "Automation in Data Collection, Curation and Annotation 627",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-06-07T00:00:00.000Z"
    },
    {
      "id": 628,
      "title": "Automation in Data Collection, Curation and Annotation 628",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-12-25T00:00:00.000Z"
    },
    {
      "id": 629,
      "title": "Automation in Data Collection, Curation and Annotation 629",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-10-24T00:00:00.000Z"
    },
    {
      "id": 630,
      "title": "Automation in Data Collection, Curation and Annotation 630",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-10-25T00:00:00.000Z"
    },
    {
      "id": 631,
      "title": "Automation in Data Collection, Curation and Annotation 631",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-12-11T00:00:00.000Z"
    },
    {
      "id": 632,
      "title": "Automation in Data Collection, Curation and Annotation 632",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-09-30T00:00:00.000Z"
    },
    {
      "id": 633,
      "title": "Automation in Data Collection, Curation and Annotation 633",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-11-07T00:00:00.000Z"
    },
    {
      "id": 634,
      "title": "Automation in Data Collection, Curation and Annotation 634",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-10-14T00:00:00.000Z"
    },
    {
      "id": 635,
      "title": "Automation in Data Collection, Curation and Annotation 635",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-05-28T00:00:00.000Z"
    },
    {
      "id": 636,
      "title": "Automation in Data Collection, Curation and Annotation 636",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-05-04T00:00:00.000Z"
    },
    {
      "id": 637,
      "title": "Automation in Data Collection, Curation and Annotation 637",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-05-09T00:00:00.000Z"
    },
    {
      "id": 638,
      "title": "Automation in Data Collection, Curation and Annotation 638",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-05-14T00:00:00.000Z"
    },
    {
      "id": 639,
      "title": "Automation in Data Collection, Curation and Annotation 639",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-03-06T00:00:00.000Z"
    },
    {
      "id": 640,
      "title": "Automation in Data Collection, Curation and Annotation 640",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-06-17T00:00:00.000Z"
    },
    {
      "id": 641,
      "title": "Automation in Data Collection, Curation and Annotation 641",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-01-02T00:00:00.000Z"
    },
    {
      "id": 642,
      "title": "Automation in Data Collection, Curation and Annotation 642",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-06-07T00:00:00.000Z"
    },
    {
      "id": 643,
      "title": "Automation in Data Collection, Curation and Annotation 643",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-10-29T00:00:00.000Z"
    },
    {
      "id": 644,
      "title": "Automation in Data Collection, Curation and Annotation 644",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-11-04T00:00:00.000Z"
    },
    {
      "id": 645,
      "title": "Automation in Data Collection, Curation and Annotation 645",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-12-19T00:00:00.000Z"
    },
    {
      "id": 646,
      "title": "Automation in Data Collection, Curation and Annotation 646",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-01-30T00:00:00.000Z"
    },
    {
      "id": 647,
      "title": "Automation in Data Collection, Curation and Annotation 647",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-10-23T00:00:00.000Z"
    },
    {
      "id": 648,
      "title": "Automation in Data Collection, Curation and Annotation 648",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-07-29T00:00:00.000Z"
    },
    {
      "id": 649,
      "title": "Automation in Data Collection, Curation and Annotation 649",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-01-22T00:00:00.000Z"
    },
    {
      "id": 650,
      "title": "Automation in Data Collection, Curation and Annotation 650",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-05-27T00:00:00.000Z"
    },
    {
      "id": 651,
      "title": "Automation in Data Collection, Curation and Annotation 651",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-12-02T00:00:00.000Z"
    },
    {
      "id": 652,
      "title": "Automation in Data Collection, Curation and Annotation 652",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-11-21T00:00:00.000Z"
    },
    {
      "id": 653,
      "title": "Automation in Data Collection, Curation and Annotation 653",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-03-03T00:00:00.000Z"
    },
    {
      "id": 654,
      "title": "Automation in Data Collection, Curation and Annotation 654",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-05-23T00:00:00.000Z"
    },
    {
      "id": 655,
      "title": "Automation in Data Collection, Curation and Annotation 655",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-07-10T00:00:00.000Z"
    },
    {
      "id": 656,
      "title": "Automation in Data Collection, Curation and Annotation 656",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-06-04T00:00:00.000Z"
    },
    {
      "id": 657,
      "title": "Automation in Data Collection, Curation and Annotation 657",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-10-13T00:00:00.000Z"
    },
    {
      "id": 658,
      "title": "Automation in Data Collection, Curation and Annotation 658",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-02-03T00:00:00.000Z"
    },
    {
      "id": 659,
      "title": "Automation in Data Collection, Curation and Annotation 659",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-04-30T00:00:00.000Z"
    },
    {
      "id": 660,
      "title": "Automation in Data Collection, Curation and Annotation 660",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-06-28T00:00:00.000Z"
    },
    {
      "id": 661,
      "title": "Automation in Data Collection, Curation and Annotation 661",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-05-31T00:00:00.000Z"
    },
    {
      "id": 662,
      "title": "Automation in Data Collection, Curation and Annotation 662",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-08-25T00:00:00.000Z"
    },
    {
      "id": 663,
      "title": "Automation in Data Collection, Curation and Annotation 663",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-04-15T00:00:00.000Z"
    },
    {
      "id": 664,
      "title": "Automation in Data Collection, Curation and Annotation 664",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-04-13T00:00:00.000Z"
    },
    {
      "id": 665,
      "title": "Automation in Data Collection, Curation and Annotation 665",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-07-19T00:00:00.000Z"
    },
    {
      "id": 666,
      "title": "Automation in Data Collection, Curation and Annotation 666",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-09-14T00:00:00.000Z"
    },
    {
      "id": 667,
      "title": "Automation in Data Collection, Curation and Annotation 667",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-11-16T00:00:00.000Z"
    },
    {
      "id": 668,
      "title": "Automation in Data Collection, Curation and Annotation 668",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-06-10T00:00:00.000Z"
    },
    {
      "id": 669,
      "title": "Automation in Data Collection, Curation and Annotation 669",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-06-13T00:00:00.000Z"
    },
    {
      "id": 670,
      "title": "Automation in Data Collection, Curation and Annotation 670",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-02-23T00:00:00.000Z"
    },
    {
      "id": 671,
      "title": "Automation in Data Collection, Curation and Annotation 671",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-04-10T00:00:00.000Z"
    },
    {
      "id": 672,
      "title": "Automation in Data Collection, Curation and Annotation 672",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-01-06T00:00:00.000Z"
    },
    {
      "id": 673,
      "title": "Automation in Data Collection, Curation and Annotation 673",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-03-29T00:00:00.000Z"
    },
    {
      "id": 674,
      "title": "Automation in Data Collection, Curation and Annotation 674",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-04-23T00:00:00.000Z"
    },
    {
      "id": 675,
      "title": "Automation in Data Collection, Curation and Annotation 675",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-02-10T00:00:00.000Z"
    },
    {
      "id": 676,
      "title": "Automation in Data Collection, Curation and Annotation 676",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-07-02T00:00:00.000Z"
    },
    {
      "id": 677,
      "title": "Automation in Data Collection, Curation and Annotation 677",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-03-17T00:00:00.000Z"
    },
    {
      "id": 678,
      "title": "Automation in Data Collection, Curation and Annotation 678",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-10-18T00:00:00.000Z"
    },
    {
      "id": 679,
      "title": "Automation in Data Collection, Curation and Annotation 679",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-07-08T00:00:00.000Z"
    },
    {
      "id": 680,
      "title": "Automation in Data Collection, Curation and Annotation 680",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-06-11T00:00:00.000Z"
    },
    {
      "id": 681,
      "title": "Automation in Data Collection, Curation and Annotation 681",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-09-10T00:00:00.000Z"
    },
    {
      "id": 682,
      "title": "Automation in Data Collection, Curation and Annotation 682",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-12-24T00:00:00.000Z"
    },
    {
      "id": 683,
      "title": "Automation in Data Collection, Curation and Annotation 683",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-01-08T00:00:00.000Z"
    },
    {
      "id": 684,
      "title": "Automation in Data Collection, Curation and Annotation 684",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-11-22T00:00:00.000Z"
    },
    {
      "id": 685,
      "title": "Automation in Data Collection, Curation and Annotation 685",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-03-02T00:00:00.000Z"
    },
    {
      "id": 686,
      "title": "Automation in Data Collection, Curation and Annotation 686",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-05-02T00:00:00.000Z"
    },
    {
      "id": 687,
      "title": "Automation in Data Collection, Curation and Annotation 687",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-02-05T00:00:00.000Z"
    },
    {
      "id": 688,
      "title": "Automation in Data Collection, Curation and Annotation 688",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-01-21T00:00:00.000Z"
    },
    {
      "id": 689,
      "title": "Automation in Data Collection, Curation and Annotation 689",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-08-27T00:00:00.000Z"
    },
    {
      "id": 690,
      "title": "Automation in Data Collection, Curation and Annotation 690",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-07-11T00:00:00.000Z"
    },
    {
      "id": 691,
      "title": "Automation in Data Collection, Curation and Annotation 691",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-12-09T00:00:00.000Z"
    },
    {
      "id": 692,
      "title": "Automation in Data Collection, Curation and Annotation 692",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-03-30T00:00:00.000Z"
    },
    {
      "id": 693,
      "title": "Automation in Data Collection, Curation and Annotation 693",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-09-10T00:00:00.000Z"
    },
    {
      "id": 694,
      "title": "Automation in Data Collection, Curation and Annotation 694",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-08-03T00:00:00.000Z"
    },
    {
      "id": 695,
      "title": "Automation in Data Collection, Curation and Annotation 695",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-03-19T00:00:00.000Z"
    },
    {
      "id": 696,
      "title": "Automation in Data Collection, Curation and Annotation 696",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-10-22T00:00:00.000Z"
    },
    {
      "id": 697,
      "title": "Automation in Data Collection, Curation and Annotation 697",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-11-18T00:00:00.000Z"
    },
    {
      "id": 698,
      "title": "Automation in Data Collection, Curation and Annotation 698",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-10-21T00:00:00.000Z"
    },
    {
      "id": 699,
      "title": "Automation in Data Collection, Curation and Annotation 699",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-07-14T00:00:00.000Z"
    },
    {
      "id": 700,
      "title": "Automation in Data Collection, Curation and Annotation 700",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-02-22T00:00:00.000Z"
    },
    {
      "id": 701,
      "title": "Automation in Data Collection, Curation and Annotation 701",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-06-02T00:00:00.000Z"
    },
    {
      "id": 702,
      "title": "Automation in Data Collection, Curation and Annotation 702",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-04-27T00:00:00.000Z"
    },
    {
      "id": 703,
      "title": "Automation in Data Collection, Curation and Annotation 703",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-05-03T00:00:00.000Z"
    },
    {
      "id": 704,
      "title": "Automation in Data Collection, Curation and Annotation 704",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-03-16T00:00:00.000Z"
    },
    {
      "id": 705,
      "title": "Automation in Data Collection, Curation and Annotation 705",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-02-15T00:00:00.000Z"
    },
    {
      "id": 706,
      "title": "Automation in Data Collection, Curation and Annotation 706",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-12-24T00:00:00.000Z"
    },
    {
      "id": 707,
      "title": "Automation in Data Collection, Curation and Annotation 707",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-02-12T00:00:00.000Z"
    },
    {
      "id": 708,
      "title": "Automation in Data Collection, Curation and Annotation 708",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-11-30T00:00:00.000Z"
    },
    {
      "id": 709,
      "title": "Automation in Data Collection, Curation and Annotation 709",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-05-26T00:00:00.000Z"
    },
    {
      "id": 710,
      "title": "Automation in Data Collection, Curation and Annotation 710",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-09-12T00:00:00.000Z"
    },
    {
      "id": 711,
      "title": "Automation in Data Collection, Curation and Annotation 711",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-03-16T00:00:00.000Z"
    },
    {
      "id": 712,
      "title": "Automation in Data Collection, Curation and Annotation 712",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-03-16T00:00:00.000Z"
    },
    {
      "id": 713,
      "title": "Automation in Data Collection, Curation and Annotation 713",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-04-04T00:00:00.000Z"
    },
    {
      "id": 714,
      "title": "Automation in Data Collection, Curation and Annotation 714",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-08-18T00:00:00.000Z"
    },
    {
      "id": 715,
      "title": "Automation in Data Collection, Curation and Annotation 715",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-01-11T00:00:00.000Z"
    },
    {
      "id": 716,
      "title": "Automation in Data Collection, Curation and Annotation 716",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-03-23T00:00:00.000Z"
    },
    {
      "id": 717,
      "title": "Automation in Data Collection, Curation and Annotation 717",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-06-23T00:00:00.000Z"
    },
    {
      "id": 718,
      "title": "Automation in Data Collection, Curation and Annotation 718",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-05-21T00:00:00.000Z"
    },
    {
      "id": 719,
      "title": "Automation in Data Collection, Curation and Annotation 719",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-08-08T00:00:00.000Z"
    },
    {
      "id": 720,
      "title": "Automation in Data Collection, Curation and Annotation 720",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-11-30T00:00:00.000Z"
    },
    {
      "id": 721,
      "title": "Automation in Data Collection, Curation and Annotation 721",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-05-30T00:00:00.000Z"
    },
    {
      "id": 722,
      "title": "Automation in Data Collection, Curation and Annotation 722",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-02-17T00:00:00.000Z"
    },
    {
      "id": 723,
      "title": "Automation in Data Collection, Curation and Annotation 723",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-12-15T00:00:00.000Z"
    },
    {
      "id": 724,
      "title": "Automation in Data Collection, Curation and Annotation 724",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-06-13T00:00:00.000Z"
    },
    {
      "id": 725,
      "title": "Automation in Data Collection, Curation and Annotation 725",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-02-15T00:00:00.000Z"
    },
    {
      "id": 726,
      "title": "Automation in Data Collection, Curation and Annotation 726",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-07-27T00:00:00.000Z"
    },
    {
      "id": 727,
      "title": "Automation in Data Collection, Curation and Annotation 727",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-03-04T00:00:00.000Z"
    },
    {
      "id": 728,
      "title": "Automation in Data Collection, Curation and Annotation 728",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-02-27T00:00:00.000Z"
    },
    {
      "id": 729,
      "title": "Automation in Data Collection, Curation and Annotation 729",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-07-06T00:00:00.000Z"
    },
    {
      "id": 730,
      "title": "Automation in Data Collection, Curation and Annotation 730",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-05-05T00:00:00.000Z"
    },
    {
      "id": 731,
      "title": "Automation in Data Collection, Curation and Annotation 731",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-05-18T00:00:00.000Z"
    },
    {
      "id": 732,
      "title": "Automation in Data Collection, Curation and Annotation 732",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-12-21T00:00:00.000Z"
    },
    {
      "id": 733,
      "title": "Automation in Data Collection, Curation and Annotation 733",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-12-12T00:00:00.000Z"
    },
    {
      "id": 734,
      "title": "Automation in Data Collection, Curation and Annotation 734",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-06-24T00:00:00.000Z"
    },
    {
      "id": 735,
      "title": "Automation in Data Collection, Curation and Annotation 735",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-07-30T00:00:00.000Z"
    },
    {
      "id": 736,
      "title": "Automation in Data Collection, Curation and Annotation 736",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-02-28T00:00:00.000Z"
    },
    {
      "id": 737,
      "title": "Automation in Data Collection, Curation and Annotation 737",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-12-04T00:00:00.000Z"
    },
    {
      "id": 738,
      "title": "Automation in Data Collection, Curation and Annotation 738",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-05-05T00:00:00.000Z"
    },
    {
      "id": 739,
      "title": "Automation in Data Collection, Curation and Annotation 739",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-09-30T00:00:00.000Z"
    },
    {
      "id": 740,
      "title": "Automation in Data Collection, Curation and Annotation 740",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-08-24T00:00:00.000Z"
    },
    {
      "id": 741,
      "title": "Automation in Data Collection, Curation and Annotation 741",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-04-02T00:00:00.000Z"
    },
    {
      "id": 742,
      "title": "Automation in Data Collection, Curation and Annotation 742",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-07-14T00:00:00.000Z"
    },
    {
      "id": 743,
      "title": "Automation in Data Collection, Curation and Annotation 743",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-01-28T00:00:00.000Z"
    },
    {
      "id": 744,
      "title": "Automation in Data Collection, Curation and Annotation 744",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-09-23T00:00:00.000Z"
    },
    {
      "id": 745,
      "title": "Automation in Data Collection, Curation and Annotation 745",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-11-17T00:00:00.000Z"
    },
    {
      "id": 746,
      "title": "Automation in Data Collection, Curation and Annotation 746",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-01-08T00:00:00.000Z"
    },
    {
      "id": 747,
      "title": "Automation in Data Collection, Curation and Annotation 747",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-12-28T00:00:00.000Z"
    },
    {
      "id": 748,
      "title": "Automation in Data Collection, Curation and Annotation 748",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2023-03-08T00:00:00.000Z"
    },
    {
      "id": 749,
      "title": "Automation in Data Collection, Curation and Annotation 749",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-12-15T00:00:00.000Z"
    },
    {
      "id": 750,
      "title": "Automation in Data Collection, Curation and Annotation 750",
      "content": "Collection, curation and annotation of data is a tedious endeavour. The challenges in this arena are two-fold: (1) To build, maintain and manage teams or crowds of skilled data-workers, allocating them a steady supply of skill-appropriate tasks, maintaining quality control and motivation. (2) To build a toolkit that enables automation of those data tasks that can be automated. This includes domain-specific and domain-agnostic tools such as data scrapers, format convertors, annotation tools, etc. There is ample scope for developing several novel machine learning based methods for such automation.\n\n    Automation requires the data to be validated subsequently with manual expert or non-expert help. To make the data usable, it often needs to be annotated using the vocabulary and practices of the underlying domain. This requires significant time of appropriate experts, by itself a difficult and costly task.\n    \n    It is reasonably obvious that the process of creating and collating data is time-consuming, tedious, and expensive. For managing teams of data-workers, a project management platform built for such purposes needs to be built and/or deployed. The Data Foundation has its target the creation of 20-25 high-impact data sets in varied domains, by employing a team of data workers for transcription, curation, annotation, etc.",
      "publishedAt": "2022-12-06T00:00:00.000Z"
    },
    {
      "id": 800,
      "title": "Universality 800",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2022-10-24T00:00:00.000Z"
    },
    {
      "id": 801,
      "title": "Universality 801",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-03-05T00:00:00.000Z"
    },
    {
      "id": 802,
      "title": "Universality 802",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-05-28T00:00:00.000Z"
    },
    {
      "id": 803,
      "title": "Universality 803",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-04-05T00:00:00.000Z"
    },
    {
      "id": 804,
      "title": "Universality 804",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-01-08T00:00:00.000Z"
    },
    {
      "id": 805,
      "title": "Universality 805",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-10-20T00:00:00.000Z"
    },
    {
      "id": 806,
      "title": "Universality 806",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-03-23T00:00:00.000Z"
    },
    {
      "id": 807,
      "title": "Universality 807",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-03-01T00:00:00.000Z"
    },
    {
      "id": 808,
      "title": "Universality 808",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-03-24T00:00:00.000Z"
    },
    {
      "id": 809,
      "title": "Universality 809",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-03-11T00:00:00.000Z"
    },
    {
      "id": 810,
      "title": "Universality 810",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2022-11-24T00:00:00.000Z"
    },
    {
      "id": 811,
      "title": "Universality 811",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-01-17T00:00:00.000Z"
    },
    {
      "id": 812,
      "title": "Universality 812",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-09-29T00:00:00.000Z"
    },
    {
      "id": 813,
      "title": "Universality 813",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-07-09T00:00:00.000Z"
    },
    {
      "id": 814,
      "title": "Universality 814",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-01-13T00:00:00.000Z"
    },
    {
      "id": 815,
      "title": "Universality 815",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2022-11-16T00:00:00.000Z"
    },
    {
      "id": 816,
      "title": "Universality 816",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2022-10-30T00:00:00.000Z"
    },
    {
      "id": 817,
      "title": "Universality 817",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-05-28T00:00:00.000Z"
    },
    {
      "id": 818,
      "title": "Universality 818",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-05-13T00:00:00.000Z"
    },
    {
      "id": 819,
      "title": "Universality 819",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-05-31T00:00:00.000Z"
    },
    {
      "id": 820,
      "title": "Universality 820",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-07-29T00:00:00.000Z"
    },
    {
      "id": 821,
      "title": "Universality 821",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-07-24T00:00:00.000Z"
    },
    {
      "id": 822,
      "title": "Universality 822",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-03-26T00:00:00.000Z"
    },
    {
      "id": 823,
      "title": "Universality 823",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-10-07T00:00:00.000Z"
    },
    {
      "id": 824,
      "title": "Universality 824",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-08-28T00:00:00.000Z"
    },
    {
      "id": 825,
      "title": "Universality 825",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2022-12-02T00:00:00.000Z"
    },
    {
      "id": 826,
      "title": "Universality 826",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-09-21T00:00:00.000Z"
    },
    {
      "id": 827,
      "title": "Universality 827",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-07-20T00:00:00.000Z"
    },
    {
      "id": 828,
      "title": "Universality 828",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-01-17T00:00:00.000Z"
    },
    {
      "id": 829,
      "title": "Universality 829",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-08-10T00:00:00.000Z"
    },
    {
      "id": 830,
      "title": "Universality 830",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-03-29T00:00:00.000Z"
    },
    {
      "id": 831,
      "title": "Universality 831",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-04-11T00:00:00.000Z"
    },
    {
      "id": 832,
      "title": "Universality 832",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-02-25T00:00:00.000Z"
    },
    {
      "id": 833,
      "title": "Universality 833",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-07-30T00:00:00.000Z"
    },
    {
      "id": 834,
      "title": "Universality 834",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-09-15T00:00:00.000Z"
    },
    {
      "id": 835,
      "title": "Universality 835",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-09-24T00:00:00.000Z"
    },
    {
      "id": 836,
      "title": "Universality 836",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-08-05T00:00:00.000Z"
    },
    {
      "id": 837,
      "title": "Universality 837",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-04-26T00:00:00.000Z"
    },
    {
      "id": 838,
      "title": "Universality 838",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-05-05T00:00:00.000Z"
    },
    {
      "id": 839,
      "title": "Universality 839",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-01-23T00:00:00.000Z"
    },
    {
      "id": 840,
      "title": "Universality 840",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2022-11-17T00:00:00.000Z"
    },
    {
      "id": 841,
      "title": "Universality 841",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-09-17T00:00:00.000Z"
    },
    {
      "id": 842,
      "title": "Universality 842",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-07-03T00:00:00.000Z"
    },
    {
      "id": 843,
      "title": "Universality 843",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-02-20T00:00:00.000Z"
    },
    {
      "id": 844,
      "title": "Universality 844",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-10-04T00:00:00.000Z"
    },
    {
      "id": 845,
      "title": "Universality 845",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-06-09T00:00:00.000Z"
    },
    {
      "id": 846,
      "title": "Universality 846",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2022-12-25T00:00:00.000Z"
    },
    {
      "id": 847,
      "title": "Universality 847",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2022-12-20T00:00:00.000Z"
    },
    {
      "id": 848,
      "title": "Universality 848",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-09-26T00:00:00.000Z"
    },
    {
      "id": 849,
      "title": "Universality 849",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2022-12-25T00:00:00.000Z"
    },
    {
      "id": 850,
      "title": "Universality 850",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-07-10T00:00:00.000Z"
    },
    {
      "id": 851,
      "title": "Universality 851",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-04-21T00:00:00.000Z"
    },
    {
      "id": 852,
      "title": "Universality 852",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-08-06T00:00:00.000Z"
    },
    {
      "id": 853,
      "title": "Universality 853",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-09-16T00:00:00.000Z"
    },
    {
      "id": 854,
      "title": "Universality 854",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-04-10T00:00:00.000Z"
    },
    {
      "id": 855,
      "title": "Universality 855",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-02-08T00:00:00.000Z"
    },
    {
      "id": 856,
      "title": "Universality 856",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-09-04T00:00:00.000Z"
    },
    {
      "id": 857,
      "title": "Universality 857",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2022-11-25T00:00:00.000Z"
    },
    {
      "id": 858,
      "title": "Universality 858",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2022-12-06T00:00:00.000Z"
    },
    {
      "id": 859,
      "title": "Universality 859",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-08-06T00:00:00.000Z"
    },
    {
      "id": 860,
      "title": "Universality 860",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-05-31T00:00:00.000Z"
    },
    {
      "id": 861,
      "title": "Universality 861",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-09-28T00:00:00.000Z"
    },
    {
      "id": 862,
      "title": "Universality 862",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2022-12-18T00:00:00.000Z"
    },
    {
      "id": 863,
      "title": "Universality 863",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-09-12T00:00:00.000Z"
    },
    {
      "id": 864,
      "title": "Universality 864",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-03-07T00:00:00.000Z"
    },
    {
      "id": 865,
      "title": "Universality 865",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2022-11-10T00:00:00.000Z"
    },
    {
      "id": 866,
      "title": "Universality 866",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-09-30T00:00:00.000Z"
    },
    {
      "id": 867,
      "title": "Universality 867",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-09-12T00:00:00.000Z"
    },
    {
      "id": 868,
      "title": "Universality 868",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-01-22T00:00:00.000Z"
    },
    {
      "id": 869,
      "title": "Universality 869",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-10-07T00:00:00.000Z"
    },
    {
      "id": 870,
      "title": "Universality 870",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-04-20T00:00:00.000Z"
    },
    {
      "id": 871,
      "title": "Universality 871",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-01-30T00:00:00.000Z"
    },
    {
      "id": 872,
      "title": "Universality 872",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-03-12T00:00:00.000Z"
    },
    {
      "id": 873,
      "title": "Universality 873",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-06-01T00:00:00.000Z"
    },
    {
      "id": 874,
      "title": "Universality 874",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2022-11-08T00:00:00.000Z"
    },
    {
      "id": 875,
      "title": "Universality 875",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-05-21T00:00:00.000Z"
    },
    {
      "id": 876,
      "title": "Universality 876",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-06-02T00:00:00.000Z"
    },
    {
      "id": 877,
      "title": "Universality 877",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-05-21T00:00:00.000Z"
    },
    {
      "id": 878,
      "title": "Universality 878",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-03-31T00:00:00.000Z"
    },
    {
      "id": 879,
      "title": "Universality 879",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-02-06T00:00:00.000Z"
    },
    {
      "id": 880,
      "title": "Universality 880",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-02-02T00:00:00.000Z"
    },
    {
      "id": 881,
      "title": "Universality 881",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2022-11-12T00:00:00.000Z"
    },
    {
      "id": 882,
      "title": "Universality 882",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2022-12-18T00:00:00.000Z"
    },
    {
      "id": 883,
      "title": "Universality 883",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2022-11-07T00:00:00.000Z"
    },
    {
      "id": 884,
      "title": "Universality 884",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-08-06T00:00:00.000Z"
    },
    {
      "id": 885,
      "title": "Universality 885",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-09-29T00:00:00.000Z"
    },
    {
      "id": 886,
      "title": "Universality 886",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-08-26T00:00:00.000Z"
    },
    {
      "id": 887,
      "title": "Universality 887",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2022-11-29T00:00:00.000Z"
    },
    {
      "id": 888,
      "title": "Universality 888",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2022-12-16T00:00:00.000Z"
    },
    {
      "id": 889,
      "title": "Universality 889",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2022-11-07T00:00:00.000Z"
    },
    {
      "id": 890,
      "title": "Universality 890",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-04-20T00:00:00.000Z"
    },
    {
      "id": 891,
      "title": "Universality 891",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-06-04T00:00:00.000Z"
    },
    {
      "id": 892,
      "title": "Universality 892",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2022-11-09T00:00:00.000Z"
    },
    {
      "id": 893,
      "title": "Universality 893",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-07-25T00:00:00.000Z"
    },
    {
      "id": 894,
      "title": "Universality 894",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-03-02T00:00:00.000Z"
    },
    {
      "id": 895,
      "title": "Universality 895",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-01-22T00:00:00.000Z"
    },
    {
      "id": 896,
      "title": "Universality 896",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-07-04T00:00:00.000Z"
    },
    {
      "id": 897,
      "title": "Universality 897",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-02-27T00:00:00.000Z"
    },
    {
      "id": 898,
      "title": "Universality 898",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-02-06T00:00:00.000Z"
    },
    {
      "id": 899,
      "title": "Universality 899",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-06-07T00:00:00.000Z"
    },
    {
      "id": 900,
      "title": "Universality 900",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-09-05T00:00:00.000Z"
    },
    {
      "id": 901,
      "title": "Universality 901",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-05-15T00:00:00.000Z"
    },
    {
      "id": 902,
      "title": "Universality 902",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-09-06T00:00:00.000Z"
    },
    {
      "id": 903,
      "title": "Universality 903",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-09-11T00:00:00.000Z"
    },
    {
      "id": 904,
      "title": "Universality 904",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2022-12-28T00:00:00.000Z"
    },
    {
      "id": 905,
      "title": "Universality 905",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2022-12-30T00:00:00.000Z"
    },
    {
      "id": 906,
      "title": "Universality 906",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-07-11T00:00:00.000Z"
    },
    {
      "id": 907,
      "title": "Universality 907",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-04-25T00:00:00.000Z"
    },
    {
      "id": 908,
      "title": "Universality 908",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-08-17T00:00:00.000Z"
    },
    {
      "id": 909,
      "title": "Universality 909",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-03-30T00:00:00.000Z"
    },
    {
      "id": 910,
      "title": "Universality 910",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-02-15T00:00:00.000Z"
    },
    {
      "id": 911,
      "title": "Universality 911",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-06-04T00:00:00.000Z"
    },
    {
      "id": 912,
      "title": "Universality 912",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-04-17T00:00:00.000Z"
    },
    {
      "id": 913,
      "title": "Universality 913",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-09-21T00:00:00.000Z"
    },
    {
      "id": 914,
      "title": "Universality 914",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-05-10T00:00:00.000Z"
    },
    {
      "id": 915,
      "title": "Universality 915",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-10-08T00:00:00.000Z"
    },
    {
      "id": 916,
      "title": "Universality 916",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-02-09T00:00:00.000Z"
    },
    {
      "id": 917,
      "title": "Universality 917",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-02-25T00:00:00.000Z"
    },
    {
      "id": 918,
      "title": "Universality 918",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-08-26T00:00:00.000Z"
    },
    {
      "id": 919,
      "title": "Universality 919",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-09-26T00:00:00.000Z"
    },
    {
      "id": 920,
      "title": "Universality 920",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-04-20T00:00:00.000Z"
    },
    {
      "id": 921,
      "title": "Universality 921",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-06-08T00:00:00.000Z"
    },
    {
      "id": 922,
      "title": "Universality 922",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-08-23T00:00:00.000Z"
    },
    {
      "id": 923,
      "title": "Universality 923",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-10-10T00:00:00.000Z"
    },
    {
      "id": 924,
      "title": "Universality 924",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-07-16T00:00:00.000Z"
    },
    {
      "id": 925,
      "title": "Universality 925",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2022-12-12T00:00:00.000Z"
    },
    {
      "id": 926,
      "title": "Universality 926",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-06-24T00:00:00.000Z"
    },
    {
      "id": 927,
      "title": "Universality 927",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-08-29T00:00:00.000Z"
    },
    {
      "id": 928,
      "title": "Universality 928",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-06-23T00:00:00.000Z"
    },
    {
      "id": 929,
      "title": "Universality 929",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-08-10T00:00:00.000Z"
    },
    {
      "id": 930,
      "title": "Universality 930",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-07-14T00:00:00.000Z"
    },
    {
      "id": 931,
      "title": "Universality 931",
      "content": "The technology-platform to be built for the Data Foundation should be usable across domains. Today, data analytics along with modern machine learning is a mature technology with several open and commercial implementations that are fast, accurate and scalable when designed for any particular application.\n\n    However, scaling data analytics across domains remains a major challenge for the data analytics community, both nationally and globally. The bottle-neck in this endeavour lies in developing a deep understanding and abstraction of the needs of the domain, along with the knowledge of possibilities of modern analytics and machine learning solutions. Such abstraction and identification of needs and possibilities requires the continuous engagement of collaborating inter-disciplinary researchers, and is not a task that can be easily packaged and off-sourced to the industry.\n    \n    The platform to be built needs to capture the aspects that are common in such engagements, while allowing flexibility and configurability in aspects that are diverse across domains",
      "publishedAt": "2023-04-16T00:00:00.000Z"
    }
  ]
}